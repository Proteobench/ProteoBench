###
# Casanovo configuration.
# Blank entries are interpreted as "None"
###

#idx_ratio = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.8, 1, 1]

# Random seed to ensure reproducible results.
random_seed: -1

#PMC decoding parameters---
PMC_enable: True
mass_control_tol: 0.1  #unit: Da, eveything decoded will be within 0.1 Da of MS/MS given mass

#--------
load_file_name: "./NON351/epoch=5-step=65000.ckpt"
# random_seed: -1 # Random
custom_ctc: False
# Spectrum processing options.
n_peaks: 800
min_mz: 1
max_mz: 6500.0
min_intensity: 0.0
remove_precursor_tol: 1  # Da
max_charge: 10
precursor_mass_tol: 50  # 50 ppm
isotope_error_range: [0, 1]

# Model architecture options.
dim_model:  400 #512
n_head: 8 #8
dim_feedforward: 1024
n_layers: 9
dropout: 0.18
dim_intensity:
custom_encoder:
max_length: 40 #70 #NON73: 32
residues:
  "G": 57.021464
  "A": 71.037114
  "S": 87.032028
  "P": 97.052764
  "V": 99.068414
  "T": 101.047670
  "C+57.021": 160.030649 # 103.009185 + 57.021464
  "L": 113.084064
  "I": 113.084064
  "N": 114.042927
  "D": 115.026943
  "Q": 128.058578
  "K": 128.094963
  "E": 129.042593
  "M": 131.040485
  "H": 137.058912
  "F": 147.068414
  "R": 156.101111
  "Y": 163.063329
  "W": 186.079313
  # Amino acid modifications.
  "M+15.995": 147.035400    # Met oxidation:   131.040485 + 15.994915
  "N+0.984": 115.026943     # Asn deamidation: 114.042927 +  0.984016
  "Q+0.984": 129.042594     # Gln deamidation: 128.058578 +  0.984016
  # N-terminal modifications.
  "+42.011": 42.010565      # Acetylation
  "+43.006": 43.005814      # Carbamylation
  "-17.027": -17.026549     # NH3 loss
  "+43.006-17.027": 25.980265
n_log: 1
tb_summarywriter:

# Neptune logger
enable_neptune: True
neptune_project: "casanovo/non-auto"
neptune_api_token: "eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIwNDlhZjhhMS1jNDM5LTRjOGItYTFlYS03N2U4ZDBhYmU0NTAifQ=="
tags: []

# Use epochs instead of iters
warmup_iters: 
max_iters: 

max_epochs: 150
warm_up_epochs: 1
learning_rate: 0.00035
weight_decay: 8e-5 #8e-5
gradient_clip_val: 2.5
gradient_clip_algorithm: "norm"
accumulate_grad_batches: 1
sync_batchnorm: False
SWA: False # StochasticWeightAveraging

# Training/inference options.
train_batch_size: 2800
predict_batch_size: 64
# n_beams: 5
n_beams: 5 # No beam search

logger:

num_sanity_val_steps: 0

train_from_scratch: False
n_nodes: 1
save_model: False
train_from_resume: False
model_save_folder_path: "./NON355"
save_weights_only: False
every_n_train_steps: 2500
temp_dir_auto: True
